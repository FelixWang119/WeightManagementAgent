"""
AI æœåŠ¡æ¨¡å—
æ”¯æŒå¤šç§æ¨¡å‹ï¼šOpenAI GPTã€é€šä¹‰åƒé—®(Qwen)
"""

import httpx
import json
from typing import AsyncGenerator, Optional, List, Dict, Any
from abc import ABC, abstractmethod
from dataclasses import dataclass
import openai

from config.settings import fastapi_settings
from utils.alert_utils import alert_error, alert_warning, AlertCategory


@dataclass
class AIResponse:
    """AI å“åº”æ•°æ®ç±»"""

    content: str
    model: str
    usage: Optional[Dict[str, int]] = None
    error: Optional[str] = None


class BaseAIClient(ABC):
    """AI å®¢æˆ·ç«¯åŸºç±»"""

    @abstractmethod
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
    ) -> AIResponse:
        """èŠå¤©å®Œæˆ"""
        pass

    @abstractmethod
    async def vision_analysis(
        self, image_url: str, prompt: str, model: Optional[str] = None
    ) -> AIResponse:
        """å›¾åƒåˆ†æï¼ˆç”¨äºé¤é£Ÿè¯†åˆ«ï¼‰"""
        pass


class OpenAIClient(BaseAIClient):
    """OpenAI å®¢æˆ·ç«¯"""

    def __init__(self):
        if not fastapi_settings.OPENAI_API_KEY:
            raise ValueError("æœªé…ç½® OPENAI_API_KEY")

        self.client = openai.AsyncOpenAI(
            api_key=fastapi_settings.OPENAI_API_KEY,
            base_url=fastapi_settings.OPENAI_API_BASE,
        )
        self.default_model = fastapi_settings.OPENAI_MODEL
        self.default_max_tokens = fastapi_settings.OPENAI_MAX_TOKENS
        self.default_temperature = fastapi_settings.OPENAI_TEMPERATURE

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
    ) -> AIResponse:
        """OpenAI èŠå¤©å®Œæˆ"""
        try:
            response = await self.client.chat.completions.create(
                model=model or self.default_model,
                messages=messages,
                max_tokens=max_tokens or self.default_max_tokens,
                temperature=temperature or self.default_temperature,
                stream=stream,
            )

            return AIResponse(
                content=response.choices[0].message.content,
                model=response.model,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                }
                if response.usage
                else None,
            )
        except Exception as e:
            # è®°å½•AIæœåŠ¡é”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="OpenAI APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or self.default_model,
                    "error": str(e),
                    "endpoint": "chat/completions",
                },
                module="ai_service.OpenAIClient",
            )
            return AIResponse(
                content="",
                model=model or self.default_model,
                error=f"OpenAI API é”™è¯¯: {str(e)}",
            )

    async def vision_analysis(
        self, image_url: str, prompt: str, model: Optional[str] = None
    ) -> AIResponse:
        """OpenAI è§†è§‰åˆ†æ"""
        try:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": image_url}},
                    ],
                }
            ]

            response = await self.client.chat.completions.create(
                model=model or "gpt-4-vision-preview",
                messages=messages,
                max_tokens=1000,
            )

            return AIResponse(
                content=response.choices[0].message.content, model=response.model
            )
        except Exception as e:
            # è®°å½•AIè§†è§‰æœåŠ¡é”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="OpenAI Vision APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or "gpt-4-vision-preview",
                    "error": str(e),
                    "endpoint": "chat/completions",
                    "feature": "vision_analysis",
                },
                module="ai_service.OpenAIClient",
            )
            return AIResponse(
                content="",
                model=model or "gpt-4-vision-preview",
                error=f"OpenAI Vision é”™è¯¯: {str(e)}",
            )


class QwenClient(BaseAIClient):
    """é€šä¹‰åƒé—®(Qwen)å®¢æˆ·ç«¯ - é˜¿é‡Œäº‘ DashScope"""

    def __init__(self):
        if not fastapi_settings.QWEN_API_KEY:
            raise ValueError("æœªé…ç½® QWEN_API_KEY")

        self.api_key = fastapi_settings.QWEN_API_KEY
        self.api_base = fastapi_settings.QWEN_API_BASE
        self.default_model = fastapi_settings.QWEN_MODEL
        self.default_max_tokens = fastapi_settings.QWEN_MAX_TOKENS
        self.default_temperature = fastapi_settings.QWEN_TEMPERATURE

        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
    ) -> AIResponse:
        """Qwen èŠå¤©å®Œæˆ - ä½¿ç”¨OpenAIå…¼å®¹æ¥å£"""
        try:
            # ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            # å¦‚æœbase_urlå·²ç»åŒ…å«compatible-mode/v1ï¼Œç›´æ¥ä½¿ç”¨
            if "compatible-mode/v1" in self.api_base:
                url = f"{self.api_base}/chat/completions"
            else:
                # å¦åˆ™æ·»åŠ compatible-mode/v1è·¯å¾„
                url = f"{self.api_base}/compatible-mode/v1/chat/completions"

            payload = {
                "model": model or self.default_model,
                "messages": messages,
                "max_tokens": max_tokens or self.default_max_tokens,
                "temperature": temperature or self.default_temperature,
            }

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    url, headers=self.headers, json=payload, timeout=60.0
                )
                response.raise_for_status()

                data = response.json()

                if "choices" in data and len(data["choices"]) > 0:
                    choice = data["choices"][0]
                    return AIResponse(
                        content=choice["message"]["content"],
                        model=data.get("model", model or self.default_model),
                        usage=data.get("usage"),
                    )
                else:
                    return AIResponse(
                        content="",
                        model=model or self.default_model,
                        error=f"Qwen API å“åº”æ ¼å¼é”™è¯¯: {data}",
                    )

        except httpx.HTTPError as e:
            # è®°å½•Qwen HTTPé”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen HTTP APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or self.default_model,
                    "error": str(e),
                    "endpoint": "chat/completions",
                    "provider": "qwen",
                },
                module="ai_service.QwenClient",
            )
            return AIResponse(
                content="",
                model=model or self.default_model,
                error=f"Qwen HTTP é”™è¯¯: {str(e)}",
            )
        except Exception as e:
            # è®°å½•Qwen APIé”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or self.default_model,
                    "error": str(e),
                    "endpoint": "chat/completions",
                    "provider": "qwen",
                },
                module="ai_service.QwenClient",
            )
            return AIResponse(
                content="",
                model=model or self.default_model,
                error=f"Qwen API é”™è¯¯: {str(e)}",
            )
        except Exception as e:
            return AIResponse(
                content="",
                model=model or self.default_model,
                error=f"Qwen API é”™è¯¯: {str(e)}",
            )

    async def vision_analysis(
        self, image_url: str, prompt: str, model: Optional[str] = None
    ) -> AIResponse:
        """Qwen å›¾åƒåˆ†æ - ä½¿ç”¨OpenAIå…¼å®¹æ¥å£"""
        try:
            # ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            # å¦‚æœbase_urlå·²ç»åŒ…å«compatible-mode/v1ï¼Œç›´æ¥ä½¿ç”¨
            if "compatible-mode/v1" in self.api_base:
                url = f"{self.api_base}/chat/completions"
            else:
                # å¦åˆ™æ·»åŠ compatible-mode/v1è·¯å¾„
                url = f"{self.api_base}/compatible-mode/v1/chat/completions"

            # å°è¯•ä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤
            vision_model = model or "qwen-vl-plus"

            # æ£€æŸ¥æ˜¯å¦æ˜¯base64 data URLï¼Œå¦‚æœä¸æ˜¯åˆ™è½¬æ¢ä¸ºbase64
            if image_url.startswith("data:image"):
                # å·²ç»æ˜¯data URLæ ¼å¼
                image_content = image_url
            else:
                # å‡è®¾æ˜¯æ–‡ä»¶URLï¼Œéœ€è¦ä¸‹è½½å¹¶è½¬æ¢ä¸ºbase64
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¸‹è½½å›¾ç‰‡
                image_content = image_url

            payload = {
                "model": vision_model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image_url", "image_url": {"url": image_content}},
                            {"type": "text", "text": prompt},
                        ],
                    }
                ],
                "max_tokens": 1000,
            }

            print(f"Visionåˆ†æè¯·æ±‚ - æ¨¡å‹: {vision_model}, URL: {url}")
            print(
                f"å›¾ç‰‡URLç±»å‹: {'data URL' if image_url.startswith('data:image') else 'æ™®é€šURL'}"
            )

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    url, headers=self.headers, json=payload, timeout=60.0
                )
                response.raise_for_status()

                data = response.json()

                if "choices" in data and len(data["choices"]) > 0:
                    choice = data["choices"][0]
                    return AIResponse(
                        content=choice["message"]["content"],
                        model=data.get("model", vision_model),
                    )
                else:
                    return AIResponse(
                        content="",
                        model=vision_model,
                        error=f"Qwen Vision å“åº”æ ¼å¼é”™è¯¯: {data}",
                    )
        except httpx.HTTPError as e:
            # è®°å½•Qwen Vision HTTPé”™è¯¯å‘Šè­¦
            error_msg = str(e)
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen Vision APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or "qwen-vl-plus",
                    "error": error_msg,
                    "endpoint": "compatible-mode/v1/chat/completions",
                    "provider": "qwen",
                    "feature": "vision_analysis",
                },
                module="ai_service.QwenClient",
            )

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹ä¸æ”¯æŒçš„é”™è¯¯
            if "400" in error_msg and "model" in error_msg.lower():
                print(f"Visionæ¨¡å‹å¯èƒ½ä¸æ”¯æŒï¼Œé”™è¯¯: {error_msg}")
                return AIResponse(
                    content="",
                    model=model or "qwen-vl-plus",
                    error=f"Qwen Vision æ¨¡å‹ä¸æ”¯æŒæˆ–é…ç½®é”™è¯¯: {error_msg}",
                )

            return AIResponse(
                content="",
                model=model or "qwen-vl-plus",
                error=f"Qwen Vision é”™è¯¯: {error_msg}",
            )
        except Exception as e:
            # è®°å½•Qwen Vision APIé”™è¯¯å‘Šè­¦
            error_msg = str(e)
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen Vision APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or "qwen-vl-plus",
                    "error": error_msg,
                    "endpoint": "compatible-mode/v1/chat/completions",
                    "provider": "qwen",
                    "feature": "vision_analysis",
                },
                module="ai_service.QwenClient",
            )
            return AIResponse(
                content="",
                model=model or "qwen-vl-plus",
                error=f"Qwen Vision é”™è¯¯: {error_msg}",
            )
        except Exception as e:
            import traceback

            traceback.print_exc()
            # è®°å½•Qwen Visioné”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen Vision APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or "qwen-vl-plus",
                    "error": str(e),
                    "endpoint": "compatible-mode/v1/chat/completions",
                    "provider": "qwen",
                    "feature": "vision_analysis",
                },
                module="ai_service.QwenClient",
            )
            return AIResponse(
                content="",
                model=model or "qwen-vl-plus",
                error=f"Qwen Vision é”™è¯¯: {str(e)}",
            )


class AIService:
    """AI æœåŠ¡ç»Ÿä¸€æ¥å£"""

    def __init__(self, provider: Optional[str] = None):
        """
        åˆå§‹åŒ– AI æœåŠ¡

        Args:
            provider: æ¨¡å‹æä¾›å•†ï¼Œ'openai' æˆ– 'qwen'ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        """
        self.provider = provider or fastapi_settings.DEFAULT_AI_PROVIDER
        self._client: Optional[BaseAIClient] = None

    def _get_client(self) -> BaseAIClient:
        """è·å–æˆ–åˆ›å»ºå®¢æˆ·ç«¯"""
        if self._client is None:
            if self.provider == "openai":
                self._client = OpenAIClient()
            elif self.provider == "qwen":
                self._client = QwenClient()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ AI æä¾›å•†: {self.provider}")
        return self._client

    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> AIResponse:
        """
        é€šç”¨èŠå¤©æ¥å£

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ [{"role": "user", "content": "..."}]
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆmax_tokens, temperature ç­‰ï¼‰

        Returns:
            AIResponse å¯¹è±¡
        """
        client = self._get_client()
        return await client.chat_completion(messages, **kwargs)

    async def analyze_image(self, image_url: str, prompt: str, **kwargs) -> AIResponse:
        """
        å›¾åƒåˆ†ææ¥å£ï¼ˆç”¨äºé¤é£Ÿè¯†åˆ«ï¼‰

        Args:
            image_url: å›¾ç‰‡ URL
            prompt: åˆ†ææç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            AIResponse å¯¹è±¡
        """
        client = self._get_client()
        return await client.vision_analysis(image_url, prompt, **kwargs)

    async def analyze_meal(self, image_url: str) -> Dict[str, Any]:
        """
        åˆ†æé¤é£Ÿç…§ç‰‡

        Args:
            image_url: é¤é£Ÿç…§ç‰‡ URL

        Returns:
            è§£æåçš„é¤é£Ÿä¿¡æ¯
        """
        prompt = """è¯·åˆ†æè¿™å¼ é¤é£Ÿç…§ç‰‡ï¼Œè¯†åˆ«é£Ÿç‰©ç§ç±»å¹¶ä¼°ç®—çƒ­é‡ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
é£Ÿç‰©åç§°: [å…·ä½“é£Ÿç‰©åç§°]
ä¸»è¦æˆåˆ†: [åˆ—å‡ºä¸»è¦é£Ÿæ]
ä¼°ç®—é‡é‡: [å…‹æ•°]
ä¼°ç®—çƒ­é‡: [åƒå¡æ•°]
è¥å…»æˆåˆ†: è›‹ç™½è´¨[X]g, ç¢³æ°´[Y]g, è„‚è‚ª[Z]g
ç½®ä¿¡åº¦: [0-1ä¹‹é—´çš„å°æ•°]

æ³¨æ„ï¼š
1. å¦‚æœæ— æ³•è¯†åˆ«ï¼Œè¯·è¯´æ˜"æ— æ³•æ¸…æ™°è¯†åˆ«"
2. çƒ­é‡ä¼°ç®—æ˜¯å¤§æ¦‚å€¼ï¼Œä»…ä¾›å‚è€ƒ
3. å¦‚æœæ˜¯ä¸­é¤ï¼Œè¯·å°½é‡ä½¿ç”¨ä¸­æ–‡èœå"""

        response = await self.analyze_image(image_url, prompt)

        if response.error:
            # è®°å½•é¤é£Ÿåˆ†æå¤±è´¥å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="é¤é£Ÿå›¾ç‰‡åˆ†æå¤±è´¥",
                details={
                    "image_url": image_url,
                    "error": response.error,
                    "model": response.model,
                },
                module="ai_service.AIService",
            )
            return {
                "success": False,
                "error": response.error,
                "raw_content": response.content,
            }

        # è§£æ AI è¿”å›çš„å†…å®¹
        content = response.content
        result = {
            "success": True,
            "model": response.model,
            "raw_content": content,
            "parsed": {},
        }

        # å°è¯•è§£æç»“æ„åŒ–æ•°æ®
        lines = content.strip().split("\n")
        for line in lines:
            if "ï¼š" in line or ":" in line:
                # ç»Ÿä¸€ä½¿ç”¨è‹±æ–‡å†’å·
                line = line.replace("ï¼š", ":")
                parts = line.split(":", 1)
                if len(parts) == 2:
                    key = parts[0].strip()
                    value = parts[1].strip()
                    result["parsed"][key] = value

        return result


# å…¨å±€ AI æœåŠ¡å®ä¾‹
ai_service = AIService()


async def test_ai():
    """æµ‹è¯• AI æœåŠ¡"""
    print("ğŸ§ª æµ‹è¯• AI æœåŠ¡...")

    # æµ‹è¯•èŠå¤©
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"},
    ]

    print(f"\nä½¿ç”¨æ¨¡å‹: {fastapi_settings.DEFAULT_AI_PROVIDER}")
    response = await ai_service.chat(messages)

    if response.error:
        print(f"âŒ é”™è¯¯: {response.error}")
    else:
        print(f"âœ… æˆåŠŸ!")
        print(f"æ¨¡å‹: {response.model}")
        print(f"å›å¤: {response.content[:100]}...")
        if response.usage:
            print(f"Token ä½¿ç”¨: {response.usage}")


if __name__ == "__main__":
    import asyncio

    asyncio.run(test_ai())

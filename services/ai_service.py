"""
AI æœåŠ¡æ¨¡å—
æ”¯æŒå¤šç§æ¨¡å‹ï¼šOpenAI GPTã€é€šä¹‰åƒé—®(Qwen)
"""

import httpx
import json
import asyncio
from typing import AsyncGenerator, Optional, List, Dict, Any, Union
from abc import ABC, abstractmethod
from dataclasses import dataclass
import openai
from openai.types.chat import ChatCompletionMessageParam
from functools import wraps

from config.settings import fastapi_settings
from utils.alert_utils import alert_error, alert_warning, AlertCategory


def retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0):
    """é‡è¯•è£…é¥°å™¨ï¼Œå¸¦æŒ‡æ•°é€€é¿"""

    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except (httpx.HTTPError, httpx.TimeoutException) as e:
                    last_exception = e
                    if attempt == max_retries - 1:
                        break

                    # æŒ‡æ•°é€€é¿å»¶è¿Ÿ
                    delay = base_delay * (2**attempt)
                    logger = args[0].__class__.__module__  # è·å–ç±»æ¨¡å—ä½œä¸ºlogger
                    print(
                        f"[{logger}] ç¬¬{attempt + 1}æ¬¡é‡è¯•å¤±è´¥ï¼Œç­‰å¾…{delay:.1f}ç§’åé‡è¯•: {str(e)}"
                    )
                    await asyncio.sleep(delay)
                except Exception as e:
                    # å…¶ä»–å¼‚å¸¸ä¸é‡è¯•
                    raise e

            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            if last_exception:
                raise last_exception
            else:
                raise Exception("é‡è¯•å¤±è´¥ï¼ŒæœªçŸ¥é”™è¯¯")

        return wrapper

    return decorator


@dataclass
class AIResponse:
    """AI å“åº”æ•°æ®ç±»"""

    content: str
    model: str
    usage: Optional[Dict[str, int]] = None
    error: Optional[str] = None

    def __post_init__(self):
        # ç¡®ä¿contentå§‹ç»ˆæ˜¯å­—ç¬¦ä¸²ï¼Œå³ä½¿ä¸ºNoneä¹Ÿè½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        if self.content is None:
            self.content = ""


class BaseAIClient(ABC):
    """AI å®¢æˆ·ç«¯åŸºç±»"""

    @abstractmethod
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
    ) -> AIResponse:
        """èŠå¤©å®Œæˆ"""
        pass

    @abstractmethod
    async def vision_analysis(
        self, image_url: str, prompt: str, model: Optional[str] = None
    ) -> AIResponse:
        """å›¾åƒåˆ†æï¼ˆç”¨äºé¤é£Ÿè¯†åˆ«ï¼‰"""
        pass


class OpenAIClient(BaseAIClient):
    """OpenAI å®¢æˆ·ç«¯"""

    def __init__(self):
        if not fastapi_settings.OPENAI_API_KEY:
            raise ValueError("æœªé…ç½® OPENAI_API_KEY")

        self.client = openai.AsyncOpenAI(
            api_key=fastapi_settings.OPENAI_API_KEY,
            base_url=fastapi_settings.OPENAI_API_BASE,
        )
        self.default_model = fastapi_settings.OPENAI_MODEL
        self.default_max_tokens = fastapi_settings.OPENAI_MAX_TOKENS
        self.default_temperature = fastapi_settings.OPENAI_TEMPERATURE

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
    ) -> AIResponse:
        """OpenAI èŠå¤©å®Œæˆ"""
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºOpenAI SDKæœŸæœ›çš„ç±»å‹
            openai_messages: List[ChatCompletionMessageParam] = []
            for msg in messages:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                # ç¡®ä¿è§’è‰²æ˜¯æœ‰æ•ˆçš„OpenAIè§’è‰²
                if role in ["system", "user", "assistant"]:
                    # ä½¿ç”¨ç±»å‹æ–­è¨€ç»•è¿‡ç±»å‹æ£€æŸ¥
                    message: ChatCompletionMessageParam = {
                        "role": role,
                        "content": content,
                    }  # type: ignore
                    openai_messages.append(message)
                else:
                    # é»˜è®¤ä½¿ç”¨userè§’è‰²
                    message: ChatCompletionMessageParam = {
                        "role": "user",
                        "content": content,
                    }  # type: ignore
                    openai_messages.append(message)

            if stream:
                # æµå¼å“åº”å¤„ç†
                response_stream = await self.client.chat.completions.create(
                    model=model or self.default_model,
                    messages=openai_messages,
                    max_tokens=max_tokens or self.default_max_tokens,
                    temperature=temperature or self.default_temperature,
                    stream=stream,
                )

                # å¯¹äºæµå¼å“åº”ï¼Œæˆ‘ä»¬æ”¶é›†æ‰€æœ‰å†…å®¹
                content_parts = []
                async for chunk in response_stream:
                    if chunk.choices[0].delta.content:
                        content_parts.append(chunk.choices[0].delta.content)

                content = "".join(content_parts)
                return AIResponse(
                    content=content,
                    model=model or self.default_model,
                )
            else:
                # éæµå¼å“åº”
                response = await self.client.chat.completions.create(
                    model=model or self.default_model,
                    messages=openai_messages,
                    max_tokens=max_tokens or self.default_max_tokens,
                    temperature=temperature or self.default_temperature,
                    stream=stream,
                )

                return AIResponse(
                    content=response.choices[0].message.content or "",
                    model=response.model,
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens,
                    }
                    if response.usage
                    else None,
                )
        except Exception as e:
            # è®°å½•AIæœåŠ¡é”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="OpenAI APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or self.default_model,
                    "error": str(e),
                    "endpoint": "chat/completions",
                },
                module="ai_service.OpenAIClient",
            )
            return AIResponse(
                content="",
                model=model or self.default_model,
                error=f"OpenAI API é”™è¯¯: {str(e)}",
            )

    async def vision_analysis(
        self, image_url: str, prompt: str, model: Optional[str] = None
    ) -> AIResponse:
        """OpenAI è§†è§‰åˆ†æ"""
        try:
            # ä½¿ç”¨ç±»å‹æ–­è¨€ç»•è¿‡ç±»å‹æ£€æŸ¥
            messages: List[ChatCompletionMessageParam] = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": image_url}},
                    ],
                }  # type: ignore
            ]

            response = await self.client.chat.completions.create(
                model=model or "gpt-4-vision-preview",
                messages=messages,
                max_tokens=1000,
            )

            return AIResponse(
                content=response.choices[0].message.content or "",
                model=response.model,
            )
        except Exception as e:
            # è®°å½•AIè§†è§‰æœåŠ¡é”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="OpenAI Vision APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or "gpt-4-vision-preview",
                    "error": str(e),
                    "endpoint": "chat/completions",
                    "feature": "vision_analysis",
                },
                module="ai_service.OpenAIClient",
            )
            return AIResponse(
                content="",
                model=model or "gpt-4-vision-preview",
                error=f"OpenAI Vision é”™è¯¯: {str(e)}",
            )


class QwenClient(BaseAIClient):
    """é€šä¹‰åƒé—®(Qwen)å®¢æˆ·ç«¯ - é˜¿é‡Œäº‘ DashScope"""

    def __init__(self):
        if not fastapi_settings.QWEN_API_KEY:
            raise ValueError("æœªé…ç½® QWEN_API_KEY")

        self.api_key = fastapi_settings.QWEN_API_KEY
        self.api_base = fastapi_settings.QWEN_API_BASE
        self.default_model = fastapi_settings.QWEN_MODEL
        self.default_max_tokens = fastapi_settings.QWEN_MAX_TOKENS
        self.default_temperature = fastapi_settings.QWEN_TEMPERATURE

        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        # è¶…æ—¶è®¾ç½®ï¼ˆç§’ï¼‰
        self.timeout = 30.0  # æ€»è¶…æ—¶
        self.connect_timeout = 10.0  # è¿æ¥è¶…æ—¶
        self.read_timeout = 20.0  # è¯»å–è¶…æ—¶

    @retry_with_backoff(max_retries=2, base_delay=1.0)
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
    ) -> AIResponse:
        """Qwen èŠå¤©å®Œæˆ - ä½¿ç”¨OpenAIå…¼å®¹æ¥å£"""
        try:
            # ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            # å¦‚æœbase_urlå·²ç»åŒ…å«compatible-mode/v1ï¼Œç›´æ¥ä½¿ç”¨
            if "compatible-mode/v1" in self.api_base:
                url = f"{self.api_base}/chat/completions"
            else:
                # å¦åˆ™æ·»åŠ compatible-mode/v1è·¯å¾„
                url = f"{self.api_base}/compatible-mode/v1/chat/completions"

            payload = {
                "model": model or self.default_model,
                "messages": messages,
                "max_tokens": max_tokens or self.default_max_tokens,
                "temperature": temperature or self.default_temperature,
            }

            async with httpx.AsyncClient(
                timeout=httpx.Timeout(
                    connect=self.connect_timeout,
                    read=self.read_timeout,
                    write=10.0,
                    pool=5.0,
                )
            ) as client:
                response = await client.post(
                    url, headers=self.headers, json=payload, timeout=self.timeout
                )
                response.raise_for_status()

                data = response.json()

                if "choices" in data and len(data["choices"]) > 0:
                    choice = data["choices"][0]
                    return AIResponse(
                        content=choice["message"]["content"],
                        model=data.get("model", model or self.default_model),
                        usage=data.get("usage"),
                    )
                else:
                    return AIResponse(
                        content="",
                        model=model or self.default_model,
                        error=f"Qwen API å“åº”æ ¼å¼é”™è¯¯: {data}",
                    )

        except httpx.HTTPError as e:
            # è®°å½•Qwen HTTPé”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen HTTP APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or self.default_model,
                    "error": str(e),
                    "endpoint": "chat/completions",
                    "provider": "qwen",
                },
                module="ai_service.QwenClient",
            )
            return AIResponse(
                content="",
                model=model or self.default_model,
                error=f"Qwen HTTP é”™è¯¯: {str(e)}",
            )
        except Exception as e:
            # è®°å½•Qwen APIé”™è¯¯å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or self.default_model,
                    "error": str(e),
                    "endpoint": "chat/completions",
                    "provider": "qwen",
                },
                module="ai_service.QwenClient",
            )
            return AIResponse(
                content="",
                model=model or self.default_model,
                error=f"Qwen API é”™è¯¯: {str(e)}",
            )

    @retry_with_backoff(max_retries=1, base_delay=2.0)
    async def vision_analysis(
        self, image_url: str, prompt: str, model: Optional[str] = None
    ) -> AIResponse:
        """Qwen å›¾åƒåˆ†æ - ä½¿ç”¨OpenAIå…¼å®¹æ¥å£"""
        try:
            # ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            # å¦‚æœbase_urlå·²ç»åŒ…å«compatible-mode/v1ï¼Œç›´æ¥ä½¿ç”¨
            if "compatible-mode/v1" in self.api_base:
                url = f"{self.api_base}/chat/completions"
            else:
                # å¦åˆ™æ·»åŠ compatible-mode/v1è·¯å¾„
                url = f"{self.api_base}/compatible-mode/v1/chat/completions"

            # å°è¯•ä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤
            vision_model = model or "qwen-vl-plus"

            # æ£€æŸ¥æ˜¯å¦æ˜¯base64 data URLï¼Œå¦‚æœä¸æ˜¯åˆ™è½¬æ¢ä¸ºbase64
            if image_url.startswith("data:image"):
                # å·²ç»æ˜¯data URLæ ¼å¼
                image_content = image_url
            else:
                # å‡è®¾æ˜¯æ–‡ä»¶URLï¼Œéœ€è¦ä¸‹è½½å¹¶è½¬æ¢ä¸ºbase64
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¸‹è½½å›¾ç‰‡
                image_content = image_url

            payload = {
                "model": vision_model,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {"type": "image_url", "image_url": {"url": image_content}},
                            {"type": "text", "text": prompt},
                        ],
                    }
                ],
                "max_tokens": 1000,
            }

            print(f"Visionåˆ†æè¯·æ±‚ - æ¨¡å‹: {vision_model}, URL: {url}")
            print(
                f"å›¾ç‰‡URLç±»å‹: {'data URL' if image_url.startswith('data:image') else 'æ™®é€šURL'}"
            )

            async with httpx.AsyncClient(
                timeout=httpx.Timeout(
                    connect=self.connect_timeout,
                    read=self.read_timeout,
                    write=10.0,
                    pool=5.0,
                )
            ) as client:
                response = await client.post(
                    url, headers=self.headers, json=payload, timeout=self.timeout
                )
                response.raise_for_status()

                data = response.json()

                if "choices" in data and len(data["choices"]) > 0:
                    choice = data["choices"][0]
                    return AIResponse(
                        content=choice["message"]["content"],
                        model=data.get("model", vision_model),
                    )
                else:
                    return AIResponse(
                        content="",
                        model=vision_model,
                        error=f"Qwen Vision å“åº”æ ¼å¼é”™è¯¯: {data}",
                    )
        except httpx.HTTPError as e:
            # è®°å½•Qwen Vision HTTPé”™è¯¯å‘Šè­¦
            error_msg = str(e)
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen Vision APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or "qwen-vl-plus",
                    "error": error_msg,
                    "endpoint": "compatible-mode/v1/chat/completions",
                    "provider": "qwen",
                    "feature": "vision_analysis",
                },
                module="ai_service.QwenClient",
            )

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹ä¸æ”¯æŒçš„é”™è¯¯
            if "400" in error_msg and "model" in error_msg.lower():
                print(f"Visionæ¨¡å‹å¯èƒ½ä¸æ”¯æŒï¼Œé”™è¯¯: {error_msg}")
                return AIResponse(
                    content="",
                    model=model or "qwen-vl-plus",
                    error=f"Qwen Vision æ¨¡å‹ä¸æ”¯æŒæˆ–é…ç½®é”™è¯¯: {error_msg}",
                )

            return AIResponse(
                content="",
                model=model or "qwen-vl-plus",
                error=f"Qwen Vision é”™è¯¯: {error_msg}",
            )
        except Exception as e:
            # è®°å½•Qwen Vision APIé”™è¯¯å‘Šè­¦
            error_msg = str(e)
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="Qwen Vision APIè°ƒç”¨å¤±è´¥",
                details={
                    "model": model or "qwen-vl-plus",
                    "error": error_msg,
                    "endpoint": "compatible-mode/v1/chat/completions",
                    "provider": "qwen",
                    "feature": "vision_analysis",
                },
                module="ai_service.QwenClient",
            )
            return AIResponse(
                content="",
                model=model or "qwen-vl-plus",
                error=f"Qwen Vision é”™è¯¯: {error_msg}",
            )


class AIService:
    """AI æœåŠ¡ç»Ÿä¸€æ¥å£"""

    def __init__(self, provider: Optional[str] = None):
        """
        åˆå§‹åŒ– AI æœåŠ¡

        Args:
            provider: æ¨¡å‹æä¾›å•†ï¼Œ'openai' æˆ– 'qwen'ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        """
        self.provider = provider or fastapi_settings.DEFAULT_AI_PROVIDER
        self._client: Optional[BaseAIClient] = None

    def _get_client(self) -> BaseAIClient:
        """è·å–æˆ–åˆ›å»ºå®¢æˆ·ç«¯"""
        if self._client is None:
            if self.provider == "openai":
                self._client = OpenAIClient()
            elif self.provider == "qwen":
                self._client = QwenClient()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ AI æä¾›å•†: {self.provider}")
        return self._client

    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> AIResponse:
        """
        é€šç”¨èŠå¤©æ¥å£

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ [{"role": "user", "content": "..."}]
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆmax_tokens, temperature ç­‰ï¼‰

        Returns:
            AIResponse å¯¹è±¡
        """
        client = self._get_client()
        return await client.chat_completion(messages, **kwargs)

    async def analyze_image(self, image_url: str, prompt: str, **kwargs) -> AIResponse:
        """
        å›¾åƒåˆ†ææ¥å£ï¼ˆç”¨äºé¤é£Ÿè¯†åˆ«ï¼‰

        Args:
            image_url: å›¾ç‰‡ URL
            prompt: åˆ†ææç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            AIResponse å¯¹è±¡
        """
        client = self._get_client()
        return await client.vision_analysis(image_url, prompt, **kwargs)

    async def analyze_meal(self, image_url: str) -> Dict[str, Any]:
        """
        åˆ†æé¤é£Ÿç…§ç‰‡

        Args:
            image_url: é¤é£Ÿç…§ç‰‡ URL

        Returns:
            è§£æåçš„é¤é£Ÿä¿¡æ¯
        """
        prompt = """è¯·åˆ†æè¿™å¼ é¤é£Ÿç…§ç‰‡ï¼Œè¯†åˆ«é£Ÿç‰©ç§ç±»å¹¶ä¼°ç®—çƒ­é‡ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤ï¼š
é£Ÿç‰©åç§°: [å…·ä½“é£Ÿç‰©åç§°]
ä¸»è¦æˆåˆ†: [åˆ—å‡ºä¸»è¦é£Ÿæ]
ä¼°ç®—é‡é‡: [å…‹æ•°]
ä¼°ç®—çƒ­é‡: [åƒå¡æ•°]
è¥å…»æˆåˆ†: è›‹ç™½è´¨[X]g, ç¢³æ°´[Y]g, è„‚è‚ª[Z]g
ç½®ä¿¡åº¦: [0-1ä¹‹é—´çš„å°æ•°]

æ³¨æ„ï¼š
1. å¦‚æœæ— æ³•è¯†åˆ«ï¼Œè¯·è¯´æ˜"æ— æ³•æ¸…æ™°è¯†åˆ«"
2. çƒ­é‡ä¼°ç®—æ˜¯å¤§æ¦‚å€¼ï¼Œä»…ä¾›å‚è€ƒ
3. å¦‚æœæ˜¯ä¸­é¤ï¼Œè¯·å°½é‡ä½¿ç”¨ä¸­æ–‡èœå"""

        response = await self.analyze_image(image_url, prompt)

        if response.error:
            # è®°å½•é¤é£Ÿåˆ†æå¤±è´¥å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="é¤é£Ÿå›¾ç‰‡åˆ†æå¤±è´¥",
                details={
                    "image_url": image_url,
                    "error": response.error,
                    "model": response.model,
                },
                module="ai_service.AIService",
            )
            return {
                "success": False,
                "error": response.error,
                "raw_content": response.content,
            }

        # è§£æ AI è¿”å›çš„å†…å®¹
        content = response.content
        result = {
            "success": True,
            "model": response.model,
            "raw_content": content,
            "parsed": {},
        }

        # å°è¯•è§£æç»“æ„åŒ–æ•°æ®
        lines = content.strip().split("\n")
        for line in lines:
            if "ï¼š" in line or ":" in line:
                # ç»Ÿä¸€ä½¿ç”¨è‹±æ–‡å†’å·
                line = line.replace("ï¼š", ":")
                parts = line.split(":", 1)
                if len(parts) == 2:
                    key = parts[0].strip()
                    value = parts[1].strip()
                    result["parsed"][key] = value

        return result

    async def generate_text(self, prompt: str, **kwargs) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆç”¨äºæ‘˜è¦ç”Ÿæˆç­‰ç®€å•æ–‡æœ¬ä»»åŠ¡ï¼‰

        Args:
            prompt: æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆmax_tokens, temperature ç­‰ï¼‰

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        """
        messages = [{"role": "user", "content": prompt}]
        response = await self.chat(messages, **kwargs)

        if response.error:
            # è®°å½•æ–‡æœ¬ç”Ÿæˆå¤±è´¥å‘Šè­¦
            alert_error(
                category=AlertCategory.AI_SERVICE,
                message="æ–‡æœ¬ç”Ÿæˆå¤±è´¥",
                details={
                    "prompt": prompt[:100] + "..." if len(prompt) > 100 else prompt,
                    "error": response.error,
                    "model": response.model,
                },
                module="ai_service.AIService",
            )
            # è¿”å›ç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨æ–¹å¯ä»¥ä¼˜é›…é™çº§
            return ""

        return response.content


# å…¨å±€ AI æœåŠ¡å®ä¾‹
ai_service = AIService()


async def test_ai():
    """æµ‹è¯• AI æœåŠ¡"""
    print("ğŸ§ª æµ‹è¯• AI æœåŠ¡...")

    # æµ‹è¯•èŠå¤©
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"},
    ]

    print(f"\nä½¿ç”¨æ¨¡å‹: {fastapi_settings.DEFAULT_AI_PROVIDER}")
    response = await ai_service.chat(messages)

    if response.error:
        print(f"âŒ é”™è¯¯: {response.error}")
    else:
        print(f"âœ… æˆåŠŸ!")
        print(f"æ¨¡å‹: {response.model}")
        print(f"å›å¤: {response.content[:100]}...")
        if response.usage:
            print(f"Token ä½¿ç”¨: {response.usage}")


if __name__ == "__main__":
    import asyncio

    asyncio.run(test_ai())

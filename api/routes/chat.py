"""
Agent å¯¹è¯ API è·¯ç”±ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
è§£å†³ AI è¶…æ—¶é—®é¢˜ï¼šå¼‚æ­¥å¤„ç† + æµå¼å“åº” + é‡è¯•æœºåˆ¶
"""

from fastapi import (
    APIRouter,
    Depends,
    HTTPException,
    status,
    BackgroundTasks,
    Body,
    UploadFile,
    File,
    Body,
)
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc, and_, func
from typing import List, Optional, Dict, Any, AsyncGenerator
from datetime import datetime, timedelta
import json
import asyncio
import httpx
import logging

from models.database import (
    get_db,
    User,
    ChatHistory,
    ConversationSummary,
    UserProfile,
    AgentConfig,
    MessageRole,
    MessageType,
)
from api.routes.user import get_current_user
from config.settings import fastapi_settings
from services.ai_service import ai_service, AIResponse

router = APIRouter()
logger = logging.getLogger(__name__)

# ============ é…ç½® ============
AI_TIMEOUT = 60.0  # AI è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
MAX_RETRIES = 2  # æœ€å¤§é‡è¯•æ¬¡æ•°


async def build_system_prompt(user: User, db: AsyncSession) -> str:
    """æ„å»ºç³»ç»Ÿ Promptï¼ˆåŒ…å«ç”¨æˆ·ç”»åƒæ•°æ®ï¼‰- ä½¿ç”¨å…¬å…±UserProfileService"""
    from services.user_profile_service import UserProfileService

    try:
        # ä½¿ç”¨UserProfileServiceè·å–å®Œæ•´ç”»åƒ
        profile_data = await UserProfileService.get_complete_profile(user.id, db)

        # ä½¿ç”¨å…¬å…±æ–¹æ³•æ„å»ºåŸºç¡€promptï¼ˆasyncæ–¹æ³•éœ€è¦awaitï¼‰
        base_prompt = await UserProfileService.format_system_prompt(profile_data)

        # æ·»åŠ é¢å¤–çš„å›å¤åŸåˆ™ï¼ˆAPIåœºæ™¯éœ€è¦æ›´è¯¦ç»†çš„å›å¤ï¼‰
        additional_rules = """
ã€å›å¤åŸåˆ™ã€‘
1. æ ¹æ®ç”¨æˆ·ç”»åƒä¸ªæ€§åŒ–å›å¤ï¼ˆå¦‚ï¼šçŸ¥é“ç”¨æˆ·æ˜¯å¤œçŒ«å­ï¼Œå¯ä»¥æé†’ä¸è¦ç†¬å¤œï¼‰
2. å…³å¿ƒç”¨æˆ·æƒ…ç»ªå’ŒçŠ¶æ€ï¼Œç»™äºˆæƒ…æ„Ÿæ”¯æŒ
3. ç»™å‡ºå…·ä½“å¯æ“ä½œçš„å»ºè®®ï¼Œä¸è¦åªè¯´ç©ºè¯
4. è§£é‡Š'ä¸ºä»€ä¹ˆ'ï¼Œè®©ç”¨æˆ·ç†è§£åŸç†
5. é€‚æ—¶é¼“åŠ±ï¼Œä½†ä¸è¦è¿‡åº¦ï¼Œä¿æŒçœŸè¯š
6. å›å¤è¯¦ç»†å……å®ï¼Œæ§åˆ¶åœ¨300-400å­—å·¦å³
7. ç»“æ„åŒ–å›å¤ï¼Œä½¿ç”¨æ®µè½åˆ†éš”ï¼Œæé«˜å¯è¯»æ€§
8. é¿å…æ•·è¡çš„å›å¤ï¼Œæ¯å¥è¯éƒ½è¦æœ‰ä»·å€¼
9. å¦‚æœç”¨æˆ·åˆ†äº«æˆæœï¼Œè¦å…·ä½“èµç¾ï¼Œä¸è¦åªè¯´'çœŸæ£’'
10. å¦‚æœç”¨æˆ·é‡åˆ°å›°éš¾ï¼Œè¦ç»™å‡ºå…·ä½“è§£å†³æ–¹æ¡ˆ"""

        return base_prompt + additional_rules

    except Exception as e:
        logger.warning(f"ä½¿ç”¨UserProfileServiceæ„å»ºpromptå¤±è´¥: %s", e)
        # Fallbackåˆ°åŸºç¡€ç‰ˆæœ¬
        return f"ä½ æ˜¯{user.nickname or 'å°åŠ©'}ï¼Œç”¨æˆ·çš„ä¸“å±ä½“é‡ç®¡ç†åŠ©æ‰‹ã€‚"


async def get_recent_context(
    user_id: int, limit: int = 10, db: Optional[AsyncSession] = None
) -> List[Dict]:
    """è·å–æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡"""
    if db is None:
        return []
    result = await db.execute(
        select(ChatHistory)
        .where(ChatHistory.user_id == user_id)
        .order_by(desc(ChatHistory.created_at))
        .limit(limit)
    )

    records = result.scalars().all()

    # è½¬æ¢ä¸º OpenAI æ ¼å¼å¹¶åè½¬é¡ºåºï¼ˆä»æ—§åˆ°æ–°ï¼‰
    context = []
    for record in reversed(records):
        context.append({"role": record.role.value, "content": record.content})

    return context


async def save_message_to_db(
    user_id: int,
    role: MessageRole,
    content: str,
    msg_type: MessageType = MessageType.TEXT,
    meta_data: Optional[Dict] = None,
):
    """åå°ä»»åŠ¡ï¼šä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“"""
    # åˆ›å»ºæ–°çš„ä¼šè¯ç”¨äºåå°ä»»åŠ¡
    from models.database import AsyncSessionLocal

    async with AsyncSessionLocal() as db:
        try:
            message = ChatHistory(
                user_id=user_id,
                role=role,
                content=content,
                msg_type=msg_type,
                meta_data=meta_data or {},
                created_at=datetime.utcnow(),
            )
            db.add(message)
            await db.commit()
        except Exception as e:
            logger.warning("ä¿å­˜æ¶ˆæ¯å¤±è´¥: %s", e)
            await db.rollback()


async def call_ai_with_retry(
    messages: List[Dict[str, str]], max_retries: int = MAX_RETRIES
) -> AIResponse:
    """å¸¦é‡è¯•æœºåˆ¶çš„ AI è°ƒç”¨"""
    last_error = None

    for attempt in range(max_retries + 1):
        try:
            # ä½¿ç”¨ asyncio.wait_for åŒ…è£… AI è°ƒç”¨ï¼Œè®¾ç½®è¶…æ—¶
            response = await asyncio.wait_for(
                ai_service.chat(messages, max_tokens=500),  # é™åˆ¶tokenæ•°ï¼ŒåŠ å¿«å“åº”
                timeout=AI_TIMEOUT,
            )

            if not response.error:
                return response

            last_error = response.error

            # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œè¿”å›é”™è¯¯
            if attempt == max_retries:
                return response

            # ç­‰å¾…åé‡è¯•
            await asyncio.sleep(1 * (attempt + 1))

        except asyncio.TimeoutError:
            last_error = f"AI è¯·æ±‚è¶…æ—¶ï¼ˆ{AI_TIMEOUT}ç§’ï¼‰"
            if attempt == max_retries:
                return AIResponse(
                    content="", model=ai_service.provider, error=last_error
                )
            await asyncio.sleep(1 * (attempt + 1))

        except Exception as e:
            last_error = str(e)
            if attempt == max_retries:
                return AIResponse(
                    content="", model=ai_service.provider, error=last_error
                )
            await asyncio.sleep(1 * (attempt + 1))

    return AIResponse(
        content="", model=ai_service.provider, error=last_error or "æœªçŸ¥é”™è¯¯"
    )


@router.post("/send")
async def send_message(
    request_data: dict = Body(...),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    å‘é€æ¶ˆæ¯ç»™ Agentï¼ˆä¼˜åŒ–ç‰ˆï¼Œä½¿ç”¨ LangChain ReAct Agentï¼‰

    - **content**: æ¶ˆæ¯å†…å®¹
    - **image_url**: å›¾ç‰‡URLï¼ˆå¯é€‰ï¼Œç”¨äºé£Ÿç‰©è¯†åˆ«ç­‰ï¼‰
    - **msg_type**: æ¶ˆæ¯ç±»å‹ï¼ˆtext/image/form ç­‰ï¼‰

    è¿”å› Agent çš„å›å¤ï¼ˆæœ€å¤§ç­‰å¾…60ç§’ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼‰
    ä½¿ç”¨ LangChain ReAct Agentï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œç”¨æˆ·ç”»åƒè®°å¿†
    """
    try:
        # ä»è¯·æ±‚æ•°æ®ä¸­æå–å‚æ•°
        content = request_data.get("content", "")
        image_url = request_data.get("image_url", "")
        msg_type = request_data.get("msg_type", "text")

        # å¦‚æœæ²¡æœ‰æ–‡æœ¬å†…å®¹ä½†æœ‰å›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤æç¤º
        if not content and image_url:
            content = "è¯·å¸®æˆ‘åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„é£Ÿç‰©"

        if not content:
            raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")

        # ç”¨æˆ·ID
        user_id = int(current_user.id)  # type: ignore[arg-type]

        # æ„å»ºæ¶ˆæ¯å†…å®¹ï¼ˆåŒ…å«å›¾ç‰‡ä¿¡æ¯ï¼‰
        full_content = content
        if image_url:
            full_content = f"{content}\n[å›¾ç‰‡:{image_url}]"

        # 1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        message = ChatHistory(
            user_id=user_id,
            role=MessageRole.USER,
            content=full_content,
            msg_type=MessageType(msg_type),
            meta_data={"image_url": image_url} if image_url else None,
            created_at=datetime.utcnow(),
        )
        db.add(message)
        await db.commit()

        # 2. è°ƒç”¨ LangChain Agentï¼ˆå¸¦ fallbackï¼‰
        try:
            from services.langchain.agents import AgentFactory

            logger.info(f"Calling AgentFactory.get_agent for user {user_id}")

            # ä½¿ç”¨ AgentFactory è·å– Agent å®ä¾‹
            agent = await AgentFactory.get_agent(user_id, db)
            result = await agent.chat(full_content)

            logger.info(f"Agent completed for user {user_id}")

            assistant_reply = result.get("response", "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ã€‚")
            structured_response = result.get(
                "structured_response",
                {"type": "text", "content": assistant_reply, "actions": []},
            )
            intermediate_steps = result.get("intermediate_steps", [])

            # è®°å½•æ—¥å¿—
            logger.info(
                f"Agent - User: {user_id}, Steps: {len(intermediate_steps)}, Type: {structured_response.get('type')}"
            )

        except Exception as agent_error:
            # Fallback åˆ°æ—§ç³»ç»Ÿï¼ˆå…¼å®¹æ€§ä¿éšœï¼‰
            logger.warning(
                f"LangChain failed, falling back to legacy AI: {agent_error}"
            )
            logger.exception(f"LangChain agent error details:")

            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            system_prompt = await build_system_prompt(current_user, db)
            recent_context = await get_recent_context(current_user.id, limit=5, db=db)

            messages = [
                {"role": "system", "content": system_prompt},
                *recent_context,
                {"role": "user", "content": content},
            ]

            # è°ƒç”¨æ—§ AI æœåŠ¡
            response = await call_ai_with_retry(messages)

            if response.error:
                # AI è°ƒç”¨å¤±è´¥ï¼Œè¿”å›å‹å¥½é”™è¯¯
                error_msg = response.error
                if "timeout" in error_msg.lower():
                    error_msg = "AI æ€è€ƒæ—¶é—´æœ‰ç‚¹é•¿ï¼Œè¯·ç¨åå†è¯•"
                elif "rate limit" in error_msg.lower():
                    error_msg = "è¯·æ±‚å¤ªé¢‘ç¹äº†ï¼Œè¯·ç¨åå†è¯•"

                return {
                    "success": False,
                    "error": error_msg,
                    "data": {
                        "content": "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ï¼Œè¯·ç¨åå†è¯•~",
                        "role": "assistant",
                        "timestamp": datetime.utcnow().isoformat(),
                        "is_error": True,
                    },
                }

            assistant_reply = response.content
            # åœ¨ fallback é€»è¾‘ä¸­å®šä¹‰ structured_response
            structured_response = {
                "type": "text",
                "content": assistant_reply,
                "actions": [],
            }
            intermediate_steps = []

        # 3. ä¿å­˜ Agent å›å¤
        reply_message = ChatHistory(
            user_id=current_user.id,
            role=MessageRole.ASSISTANT,
            content=assistant_reply,
            msg_type=MessageType.TEXT,
            created_at=datetime.utcnow(),
        )
        db.add(reply_message)
        await db.commit()

        # 4. è¿”å›å“åº”ï¼ˆæ”¯æŒç»“æ„åŒ–æ¶ˆæ¯ï¼‰
        return {
            "success": True,
            "data": {
                "content": assistant_reply,
                "role": "assistant",
                "timestamp": datetime.utcnow().isoformat(),
                "model": "langchain-agent-v2",
                "message_type": structured_response.get("type", "text"),
                "actions": structured_response.get("actions", []),
                "intermediate_steps": intermediate_steps if intermediate_steps else [],
            },
        }

    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}",
        )


@router.post("/upload-image")
async def upload_image(
    file: UploadFile = File(...), current_user: User = Depends(get_current_user)
):
    """
    ä¸Šä¼ å›¾ç‰‡ç”¨äºèŠå¤©
    è¿”å›å›¾ç‰‡çš„è®¿é—®URL
    """
    import os
    import uuid
    from pathlib import Path

    try:
        # 1. éªŒè¯æ–‡ä»¶ç±»å‹ï¼ˆç™½åå•ï¼‰
        allowed_types = {"image/jpeg", "image/png", "image/gif", "image/webp"}
        if file.content_type not in allowed_types:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„å›¾ç‰‡ç±»å‹")

        # 2. éªŒè¯æ–‡ä»¶æ‰©å±•åï¼ˆç™½åå•ï¼‰
        allowed_extensions = {".jpg", ".jpeg", ".png", ".gif", ".webp"}
        ext = Path(file.filename).suffix.lower() if file.filename else ".jpg"
        if ext not in allowed_extensions:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å")

        # 3. é™åˆ¶æ–‡ä»¶å¤§å°ï¼ˆæœ€å¤§5MBï¼‰
        max_size = 5 * 1024 * 1024  # 5MB
        content = await file.read()
        if len(content) > max_size:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡5MB")

        # 4. ç”Ÿæˆå®‰å…¨æ–‡ä»¶åï¼ˆé˜²æ­¢è·¯å¾„éå†æ”»å‡»ï¼‰
        safe_ext = ext.lstrip(".")  # ç§»é™¤å¯èƒ½çš„å‰å¯¼ç‚¹
        filename = f"{current_user.id}_{uuid.uuid4().hex}.{safe_ext}"

        # 5. ä¿å­˜æ–‡ä»¶åˆ° uploads ç›®å½•
        upload_dir = fastapi_settings.UPLOAD_DIR
        os.makedirs(upload_dir, exist_ok=True)

        # 6. ä½¿ç”¨ç»å¯¹è·¯å¾„å¹¶éªŒè¯è·¯å¾„å®‰å…¨
        filepath = os.path.abspath(os.path.join(upload_dir, filename))
        if not filepath.startswith(os.path.abspath(upload_dir)):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶è·¯å¾„")

        # 7. å†™å…¥æ–‡ä»¶
        with open(filepath, "wb") as f:
            f.write(content)

        # è¿”å›å›¾ç‰‡URL
        image_url = f"/uploads/{filename}"

        return {"success": True, "url": image_url, "filename": filename}

    except HTTPException:
        raise
    except Exception as e:
        logger.warning("ä¸Šä¼ å›¾ç‰‡å¤±è´¥: %s", e)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {str(e)}",
        )


@router.post("/send-async")
async def send_message_async(
    content: str,
    msg_type: str = "text",
    background_tasks: BackgroundTasks = None,  # type: ignore[assignment]
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    å¼‚æ­¥å‘é€æ¶ˆæ¯ï¼ˆç«‹å³è¿”å›ï¼Œåå°å¤„ç† AIï¼‰

    é€‚åˆä¸éœ€è¦ç«‹å³å¾—åˆ°å›å¤çš„åœºæ™¯ï¼Œå¦‚ï¼š
    - ç”¨æˆ·åªæ˜¯æƒ³è®°å½•ä¿¡æ¯
    - æ‰¹é‡å¤„ç†æ¶ˆæ¯
    - éå³æ—¶å¯¹è¯

    è¿”å›ï¼šæ¶ˆæ¯å·²æ¥æ”¶ï¼Œæ­£åœ¨å¤„ç†
    """
    try:
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        message = ChatHistory(
            user_id=current_user.id,
            role=MessageRole.USER,
            content=content,
            msg_type=MessageType(msg_type),
            created_at=datetime.utcnow(),
        )
        db.add(message)
        await db.commit()

        # è§¦å‘åå° AI å¤„ç†
        # TODO: å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ çœŸæ­£çš„åå°å¤„ç†é€»è¾‘

        return {
            "success": True,
            "message": "æ¶ˆæ¯å·²æ¥æ”¶ï¼ŒAI æ­£åœ¨æ€è€ƒä¸­...",
            "data": {
                "status": "processing",
                "check_url": f"/api/chat/history?limit=1",
                "timestamp": datetime.utcnow().isoformat(),
            },
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å‘é€æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}",
        )


@router.get("/stream")
async def stream_chat(
    content: str,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
) -> StreamingResponse:
    """
    æµå¼å¯¹è¯ï¼ˆSSE - Server-Sent Eventsï¼‰

    AI é€å­—è¿”å›ï¼Œç”¨æˆ·å¯ä»¥å®æ—¶çœ‹åˆ°ç”Ÿæˆè¿‡ç¨‹
    é€‚åˆéœ€è¦å³æ—¶åé¦ˆçš„åœºæ™¯

    ä½¿ç”¨æ–¹å¼ï¼š
    ```javascript
    const eventSource = new EventSource('/api/chat/stream?content=ä½ å¥½');
    eventSource.onmessage = (event) => {
        console.log(event.data);
    };
    ```

    æ³¨æ„ï¼šç›®å‰ä»…æ”¯æŒ OpenAIï¼ŒQwen æš‚ä¸æ”¯æŒæµå¼
    """
    # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    message = ChatHistory(
        user_id=current_user.id,
        role=MessageRole.USER,
        content=content,
        msg_type=MessageType.TEXT,
        created_at=datetime.utcnow(),
    )
    db.add(message)
    await db.commit()

    # æ„å»ºä¸Šä¸‹æ–‡
    system_prompt = await build_system_prompt(current_user, db)
    recent_context = await get_recent_context(current_user.id, limit=5, db=db)

    messages = [
        {"role": "system", "content": system_prompt},
        *recent_context,
        {"role": "user", "content": content},
    ]

    async def generate_stream() -> AsyncGenerator[str, None]:
        """ç”Ÿæˆæµå¼å“åº”"""
        full_content = ""

        try:
            # ä½¿ç”¨ OpenAI æµå¼ API
            if ai_service.provider == "openai":
                from openai import AsyncOpenAI

                client = AsyncOpenAI(
                    api_key=fastapi_settings.OPENAI_API_KEY,
                    base_url=fastapi_settings.OPENAI_API_BASE,
                )

                stream = await client.chat.completions.create(
                    model=fastapi_settings.OPENAI_MODEL,
                    messages=messages,
                    max_tokens=500,
                    stream=True,
                )

                async for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content_chunk = chunk.choices[0].delta.content
                        full_content += content_chunk
                        yield f"data: {json.dumps({'content': content_chunk, 'done': False})}\n\n"

                # ä¿å­˜å®Œæ•´å›å¤
                await save_message_to_db(
                    user_id=current_user.id,
                    role=MessageRole.ASSISTANT,
                    content=full_content,
                    msg_type=MessageType.TEXT,
                )

                yield f"data: {json.dumps({'content': '', 'done': True})}\n\n"

            else:
                # Qwen æš‚ä¸æ”¯æŒæµå¼ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼
                yield f"data: {json.dumps({'content': 'æµå¼å“åº”å½“å‰ä»…æ”¯æŒ OpenAI æ¨¡å‹', 'done': False})}\n\n"

                response = await call_ai_with_retry(messages)
                if response.content:
                    yield f"data: {json.dumps({'content': response.content, 'done': False})}\n\n"

                    await save_message_to_db(
                        user_id=current_user.id,
                        role=MessageRole.ASSISTANT,
                        content=response.content,
                        msg_type=MessageType.TEXT,
                    )

                yield f"data: {json.dumps({'content': '', 'done': True})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


@router.post("/stream")
async def stream_chat_post(
    content: str = Body(..., description="ç”¨æˆ·æ¶ˆæ¯å†…å®¹"),
    images: Optional[List[str]] = Body(None, description="å›¾ç‰‡URLåˆ—è¡¨"),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
) -> StreamingResponse:
    """
    æµå¼å¯¹è¯ï¼ˆPOSTç‰ˆæœ¬ - æ›´å¥½çš„æ”¯æŒå›¾ç‰‡å’Œå†…å®¹ç±»å‹ï¼‰

    ä½¿ç”¨æ–¹å¼ï¼š
    ```javascript
    const response = await fetch('/api/chat/stream', {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${token}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            content: 'ç”¨æˆ·æ¶ˆæ¯',
            images: ['image_url_1', 'image_url_2']  // å¯é€‰
        })
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        console.log(chunk); // å¤„ç†æµå¼æ•°æ®
    }
    ```

    æ”¯æŒçš„å†…å®¹ç±»å‹ï¼š
    - text: æ™®é€šæ–‡æœ¬
    - image: å›¾ç‰‡
    - card: å¡ç‰‡ï¼ˆå›¾è¡¨ã€æ•°æ®å±•ç¤ºï¼‰
    - quick_actions: å¿«æ·æ“ä½œæŒ‰é’®
    """

    # æ„å»ºç”¨æˆ·æ¶ˆæ¯å†…å®¹
    user_content = content
    if images:
        # å›¾ç‰‡ä»¥ markdown æ ¼å¼åµŒå…¥
        image_markdown = "\n".join([f"![å›¾ç‰‡]({img})" for img in images])
        user_content = f"{content}\n\n{image_markdown}"

    # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    message = ChatHistory(
        user_id=current_user.id,
        role=MessageRole.USER,
        content=user_content,
        msg_type=MessageType.TEXT,
        meta_data={"images": images} if images else None,
        created_at=datetime.utcnow(),
    )
    db.add(message)
    await db.commit()

    # æ„å»ºä¸Šä¸‹æ–‡
    system_prompt = await build_system_prompt(current_user, db)
    recent_context = await get_recent_context(current_user.id, limit=5, db=db)

    messages = [
        {"role": "system", "content": system_prompt},
        *recent_context,
        {"role": "user", "content": user_content},
    ]

    async def generate_stream() -> AsyncGenerator[str, None]:
        """ç”Ÿæˆæµå¼å“åº”ï¼Œæ”¯æŒå¤šç§å†…å®¹ç±»å‹"""
        full_content = ""
        current_type = "text"

        try:
            if ai_service.provider == "openai":
                from openai import AsyncOpenAI

                client = AsyncOpenAI(
                    api_key=fastapi_settings.OPENAI_API_KEY,
                    base_url=fastapi_settings.OPENAI_API_BASE,
                )

                stream = await client.chat.completions.create(
                    model=fastapi_settings.OPENAI_MODEL,
                    messages=messages,
                    max_tokens=1000,
                    stream=True,
                )

                async for chunk in stream:
                    if chunk.choices[0].delta.content:
                        content_chunk = chunk.choices[0].delta.content
                        full_content += content_chunk

                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šæ ‡è®°ï¼ˆç”¨äºè¯†åˆ«å†…å®¹ç±»å‹ï¼‰
                        if content_chunk.startswith("[IMAGE:"):
                            # å›¾ç‰‡æ ‡è®°
                            image_url = content_chunk[7:-1].strip()
                            yield f"data: {json.dumps({'type': 'image', 'content': image_url, 'done': False})}\n\n"
                        elif content_chunk.startswith("[CARD:"):
                            # å¡ç‰‡æ ‡è®°
                            card_data = content_chunk[6:-1].strip()
                            yield f"data: {json.dumps({'type': 'card', 'content': card_data, 'done': False})}\n\n"
                        elif content_chunk.startswith("[ACTIONS:"):
                            # å¿«æ·æ“ä½œ
                            actions = content_chunk[9:-1].strip()
                            yield f"data: {json.dumps({'type': 'quick_actions', 'content': actions, 'done': False})}\n\n"
                        else:
                            # æ™®é€šæ–‡æœ¬
                            yield f"data: {json.dumps({'type': 'text', 'content': content_chunk, 'done': False})}\n\n"

                # ä¿å­˜å®Œæ•´å›å¤
                await save_message_to_db(
                    user_id=current_user.id,
                    role=MessageRole.ASSISTANT,
                    content=full_content,
                    msg_type=MessageType.TEXT,
                )

                yield f"data: {json.dumps({'type': 'done', 'content': '', 'done': True})}\n\n"

            else:
                # Qwen æˆ–å…¶ä»–æ¨¡å‹
                yield f"data: {json.dumps({'type': 'info', 'content': 'æµå¼å“åº”å½“å‰ä»…æ”¯æŒ OpenAI æ¨¡å‹ï¼Œå°†ä½¿ç”¨æ™®é€šæ¨¡å¼', 'done': False})}\n\n"

                response = await call_ai_with_retry(messages)
                if response.content:
                    full_content = response.content
                    yield f"data: {json.dumps({'type': 'text', 'content': full_content, 'done': False})}\n\n"

                    await save_message_to_db(
                        user_id=current_user.id,
                        role=MessageRole.ASSISTANT,
                        content=full_content,
                        msg_type=MessageType.TEXT,
                    )

                yield f"data: {json.dumps({'type': 'done', 'content': '', 'done': True})}\n\n"

        except Exception as e:
            error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
            yield f"data: {json.dumps({'type': 'error', 'content': error_msg, 'done': True})}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


@router.get("/history")
async def get_chat_history(
    limit: int = 50,
    before_id: Optional[int] = None,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """è·å–å¯¹è¯å†å²"""
    query = select(ChatHistory).where(ChatHistory.user_id == current_user.id)

    if before_id:
        query = query.where(ChatHistory.id < before_id)

    query = query.order_by(desc(ChatHistory.created_at)).limit(limit)

    result = await db.execute(query)
    records = result.scalars().all()

    return {
        "success": True,
        "count": len(records),
        "data": [
            {
                "id": r.id,
                "role": r.role.value,
                "content": r.content,
                "msg_type": r.msg_type.value,
                "created_at": r.created_at.isoformat(),
            }
            for r in reversed(records)
        ],
    }


@router.post("/clear")
async def clear_chat_history(
    current_user: User = Depends(get_current_user), db: AsyncSession = Depends(get_db)
):
    """æ¸…ç©ºå¯¹è¯å†å²"""
    result = await db.execute(
        select(ChatHistory).where(ChatHistory.user_id == current_user.id)
    )
    records = result.scalars().all()

    for record in records:
        await db.delete(record)

    await db.commit()

    return {
        "success": True,
        "message": f"å·²æ¸…ç©º {len(records)} æ¡å¯¹è¯è®°å½•",
        "deleted_count": len(records),
    }


@router.get("/context")
async def get_context_info(
    current_user: User = Depends(get_current_user), db: AsyncSession = Depends(get_db)
):
    """è·å–å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    result = await db.execute(
        select(func.count(ChatHistory.id)).where(ChatHistory.user_id == current_user.id)
    )
    total_messages = result.scalar()

    today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
    result = await db.execute(
        select(func.count(ChatHistory.id)).where(
            and_(
                ChatHistory.user_id == current_user.id, ChatHistory.created_at >= today
            )
        )
    )
    today_messages = result.scalar()

    system_prompt = await build_system_prompt(current_user, db)

    return {
        "success": True,
        "data": {
            "total_messages": total_messages,
            "today_messages": today_messages,
            "ai_provider": ai_service.provider,
            "ai_timeout": AI_TIMEOUT,
            "max_retries": MAX_RETRIES,
            "system_prompt_preview": system_prompt[:300] + "..."
            if len(system_prompt) > 300
            else system_prompt,
        },
    }


# ============ æ¯æ—¥å»ºè®®åŠŸèƒ½ ============

from datetime import timedelta
from functools import lru_cache
from typing import Optional
import time

# ç¼“å­˜ç”¨æˆ·çš„æ¯æ—¥å»ºè®®ï¼ˆå¸¦è¿‡æœŸæœºåˆ¶ï¼‰
_suggestion_cache: Dict[int, Dict[str, Any]] = {}
_CACHE_MAX_SIZE = 100  # æœ€å¤§ç¼“å­˜ç”¨æˆ·æ•°
_CACHE_EXPIRE_SECONDS = 3600  # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆ1å°æ—¶ï¼‰


def _get_cached_suggestion(user_id: int) -> Optional[Dict[str, Any]]:
    """è·å–ç¼“å­˜çš„å»ºè®®ï¼ˆå¸¦è¿‡æœŸæ£€æŸ¥ï¼‰"""
    if user_id not in _suggestion_cache:
        return None

    cached = _suggestion_cache[user_id]
    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
    cached_time = cached.get("_cached_at", 0)
    if time.time() - cached_time > _CACHE_EXPIRE_SECONDS:
        del _suggestion_cache[user_id]
        return None

    return cached.get("suggestion")


def _set_cached_suggestion(user_id: int, suggestion: Dict[str, Any]) -> None:
    """è®¾ç½®ç¼“å­˜çš„å»ºè®®ï¼ˆå¸¦å¤§å°é™åˆ¶ï¼‰"""
    # ç®€å•çš„ç¼“å­˜æ¸…ç†ï¼šå¦‚æœè¶…è¿‡æœ€å¤§sizeï¼Œåˆ é™¤æœ€æ—©çš„ç¼“å­˜
    if len(_suggestion_cache) >= _CACHE_MAX_SIZE:
        oldest_key = min(
            _suggestion_cache.keys(),
            key=lambda k: _suggestion_cache[k].get("_cached_at", 0),
        )
        del _suggestion_cache[oldest_key]

    _suggestion_cache[user_id] = {
        "suggestion": suggestion,
        "date": suggestion.get("created_at", "")[:10],  # æå–æ—¥æœŸ
        "_cached_at": time.time(),
    }


@router.get("/daily-suggestion")
async def get_daily_suggestion(
    refresh: bool = False,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    è·å–AIç”Ÿæˆçš„æ¯æ—¥1æ¡å»ºè®®

    - **refresh**: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°è·å–æ–°å»ºè®®
    - è¿”å›1æ¡ä¸ªæ€§åŒ–å»ºè®®ï¼ŒåŒ…å«å»ºè®®æ–‡æœ¬ã€ç±»å‹ã€ç›¸å…³æ“ä½œ
    """
    from datetime import date
    from models.database import (
        WeightRecord,
        MealRecord,
        ExerciseRecord,
        WaterRecord,
        MealType,
    )

    today = date.today()
    cache_key = current_user.id

    # æ£€æŸ¥ç¼“å­˜ï¼ˆé™¤éå¼ºåˆ¶åˆ·æ–°ï¼‰- ä½¿ç”¨å¸¦è¿‡æœŸæ£€æŸ¥çš„æ–°å‡½æ•°
    if not refresh:
        cached = _get_cached_suggestion(cache_key)
        if cached is not None:
            return {"success": True, "suggestion": cached, "cached": True}

    try:
        # 1. è·å–ä»Šæ—¥ä½“é‡è®°å½•
        today_start = datetime.combine(today, datetime.min.time())
        today_end = datetime.combine(today, datetime.max.time())

        weight_result = await db.execute(
            select(WeightRecord)
            .where(
                and_(
                    WeightRecord.user_id == current_user.id,
                    WeightRecord.record_time >= today_start,
                    WeightRecord.record_time <= today_end,
                )
            )
            .order_by(WeightRecord.record_time.desc())
        )
        today_weight = weight_result.scalar_one_or_none()

        # 2. è·å–ä»Šæ—¥é¤é£Ÿè®°å½•
        meal_result = await db.execute(
            select(MealRecord).where(
                and_(
                    MealRecord.user_id == current_user.id,
                    MealRecord.record_time >= today_start,
                    MealRecord.record_time <= today_end,
                )
            )
        )
        today_meals = meal_result.scalars().all()

        meal_summary = {
            "breakfast": False,
            "lunch": False,
            "dinner": False,
            "snack": False,
            "total_calories": 0,
        }
        for meal in today_meals:
            meal_type = meal.meal_type.value if meal.meal_type else None
            if meal_type:
                meal_summary[meal_type] = True
                meal_summary["total_calories"] += meal.total_calories or 0

        # 3. è·å–ä»Šæ—¥è¿åŠ¨è®°å½•
        exercise_result = await db.execute(
            select(ExerciseRecord).where(
                and_(
                    ExerciseRecord.user_id == current_user.id,
                    ExerciseRecord.record_time >= today_start,
                    ExerciseRecord.record_time <= today_end,
                )
            )
        )
        today_exercises = exercise_result.scalars().all()
        total_exercise_minutes = sum(e.duration_minutes or 0 for e in today_exercises)

        # 4. è·å–ä»Šæ—¥é¥®æ°´è®°å½•
        water_result = await db.execute(
            select(WaterRecord).where(
                and_(
                    WaterRecord.user_id == current_user.id,
                    WaterRecord.record_time >= today_start,
                    WaterRecord.record_time <= today_end,
                )
            )
        )
        today_waters = water_result.scalars().all()
        total_water_ml = sum(w.amount_ml or 0 for w in today_waters)

        # 5. æ„å»ºAIæç¤ºè¯
        current_hour = datetime.now().hour
        time_of_day = (
            "æ—©ä¸Š"
            if 5 <= current_hour < 11
            else "ä¸­åˆ"
            if 11 <= current_hour < 14
            else "ä¸‹åˆ"
            if 14 <= current_hour < 18
            else "æ™šä¸Š"
        )

        prompt = f"""ä½ æ˜¯ç”¨æˆ·çš„ä¸“å±å¥åº·é¡¾é—®ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ•°æ®ï¼Œä¸º{time_of_day}çš„ä»–/å¥¹ç”Ÿæˆ1æ¡è½»æ¾æœ‰è¶£çš„å»ºè®®ï¼ˆ30-50å­—ï¼‰ã€‚

ã€ç”¨æˆ·ä»Šæ—¥æ•°æ®ã€‘
- ä½“é‡: {f"{today_weight.weight}kg" if today_weight else "æœªè®°å½•ğŸ“"}
- é¥®é£Ÿ: {"âœ… å·²è®°å½•" if meal_summary["total_calories"] > 0 else "å¾…è®°å½•ğŸ½ï¸"}
- è¿åŠ¨: {f"{total_exercise_minutes}åˆ†é’ŸğŸƒ" if total_exercise_minutes > 0 else "å¾…è®°å½•ğŸ’ª"}
- é¥®æ°´: {f"{total_water_ml}mlğŸ’§" if total_water_ml > 0 else "å¾…è®°å½•ğŸ¥›"}
- å½“å‰æ—¶é—´: {time_of_day}

ã€è¯·é€‰æ‹©ä»¥ä¸‹é£æ ¼ä¹‹ä¸€ï¼Œæ¯æ¬¡éšæœºé€‰ï¼Œä¸è¦é‡å¤ã€‘ï¼ˆå¿…é¡»é€‰1ç§ï¼‰ï¼š
1ï¸âƒ£ è½»æ¾ç§‘æ™® - åˆ†äº«1ä¸ªæœ‰è¶£çš„å°çŸ¥è¯†ï¼ˆä¸è¦è¯´æ•™ï¼‰
2ï¸âƒ£ æ¸©æš–é¼“åŠ± - ä¸€å¥æ‰“æ°”çš„è¯ï¼ˆä¸è¦é¸¡æ±¤ï¼‰
3ï¸âƒ£ ç”Ÿæ´»æŠ€å·§ - 1ä¸ªå®ç”¨å°å¦™æ‹›
4ï¸âƒ£ å†·çŸ¥è¯† - æ„æƒ³ä¸åˆ°çš„å¥åº·å†·çŸ¥è¯†
5ï¸âƒ£ ä»Šæ—¥å°äº‹ - å»ºè®®åš1ä»¶ç®€å•å°äº‹ï¼ˆä¸è¶…è¿‡5ä¸ªå­—çš„åŠ¨ä½œï¼‰
6ï¸âƒ£ è¶£å‘³é—®ç­” - é—®1ä¸ªæœ‰è¶£çš„é€‰æ‹©é¢˜

ã€ä¸åŒé£æ ¼ç¤ºä¾‹ã€‘ï¼š
- è½»æ¾ç§‘æ™®: "ä½ çŸ¥é“å—ï¼Ÿå’€åš¼20æ¬¡ä»¥ä¸Šèƒ½è®©å¤§è„‘åŠæ—¶æ”¶åˆ°é¥±è…¹ä¿¡å·~"
- æ¸©æš–é¼“åŠ±: "ä»Šå¤©ä¹Ÿåœ¨åŠªåŠ›çš„ä½ ï¼ŒçœŸçš„å¾ˆæ£’ï¼ğŸŒŸ"
- ç”Ÿæ´»æŠ€å·§: "é¥­å‰å–ä¸€å°æ¯æ°´ï¼Œå¯ä»¥å°‘åƒçº¦50kcalå“¦~"
- å†·çŸ¥è¯†: "ç¡ä¸å¤Ÿä¼šè®©äººæ›´æƒ³åƒé«˜çƒ­é‡é£Ÿç‰©ï¼Œè¿™å°±æ˜¯'ç¡çœ å€ºåŠ¡'ğŸ˜´"
- ä»Šæ—¥å°äº‹: "ç«™èµ·æ¥ä¼¸ä¸ªæ‡’è…°"
- è¶£å‘³é—®ç­”: "ä»Šå¤©åƒå’¸è¿˜æ˜¯åƒæ·¡ï¼ŸğŸ¥—"

ã€é‡è¦è§„åˆ™ã€‘ï¼š
âš ï¸ ä¸è¦æ¯æ¬¡éƒ½é€‰"æ‰“å¡æé†’"
âš ï¸ ä¸è¦è¿ç»­2æ¬¡é€‰åŒä¸€ç§é£æ ¼
âš ï¸ ä¸è¦è¯´"è®°å¾—è®°å½•""è¦åŠ æ²¹å“¦"è¿™ç±»è¯
âš ï¸ 30-50å­—ï¼Œç®€æ´æœ‰åŠ›
âš ï¸ ç”¨emojiå¢åŠ è¶£å‘³æ€§ï¼ˆ1-2ä¸ªï¼‰

ç›´æ¥è¾“å‡ºå»ºè®®å†…å®¹ï¼Œä¸éœ€è¦è§£é‡Šã€‚"""

        # 6. è°ƒç”¨AIç”Ÿæˆå»ºè®®
        ai_response = await ai_service.chat(
            [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸“ä¸šçš„ä½“é‡ç®¡ç†é¡¾é—®ï¼Œå–„äºç»™å‡ºç®€æ´å®ç”¨çš„å»ºè®®ã€‚",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=150,
            temperature=0.8,
        )

        if ai_response.error:
            raise Exception(ai_response.error)

        suggestion_content = ai_response.content.strip()

        # 7. ç¡®å®šå»ºè®®ç±»å‹å’Œæ“ä½œ
        suggestion_type = "general"
        action_text = "çŸ¥é“äº†"
        action_target = ""

        # æ ¹æ®å†…å®¹å’Œæ•°æ®çŠ¶æ€ç¡®å®šç±»å‹å’Œå»ºè®®æ“ä½œ
        if not today_weight and "ä½“é‡" in suggestion_content:
            suggestion_type = "weight"
            action_text = "è®°å½•ä½“é‡"
            action_target = "weight.html"
        elif not meal_summary["breakfast"] and "æ—©é¤" in suggestion_content:
            suggestion_type = "meal"
            action_text = "è®°å½•æ—©é¤"
            action_target = "meal.html?type=breakfast"
        elif not meal_summary["lunch"] and "åˆé¤" in suggestion_content:
            suggestion_type = "meal"
            action_text = "è®°å½•åˆé¤"
            action_target = "meal.html?type=lunch"
        elif not meal_summary["dinner"] and "æ™šé¤" in suggestion_content:
            suggestion_type = "meal"
            action_text = "è®°å½•æ™šé¤"
            action_target = "meal.html?type=dinner"
        elif total_water_ml < 1000 and (
            "æ°´" in suggestion_content or "é¥®æ°´" in suggestion_content
        ):
            suggestion_type = "water"
            action_text = "è®°å½•é¥®æ°´"
            action_target = "water.html"
        elif total_exercise_minutes < 30 and (
            "è¿åŠ¨" in suggestion_content or "æ´»åŠ¨" in suggestion_content
        ):
            suggestion_type = "exercise"
            action_text = "è®°å½•è¿åŠ¨"
            action_target = "exercise.html"

        # 8. æ„å»ºå»ºè®®å¯¹è±¡
        suggestion = {
            "id": f"sugg_{current_user.id}_{today.isoformat()}_{datetime.now().timestamp()}",
            "content": suggestion_content,
            "type": suggestion_type,
            "priority": "high"
            if not today_weight or meal_summary["total_calories"] < 500
            else "medium",
            "action_text": action_text,
            "action_type": "navigate" if action_target else "none",
            "action_target": action_target,
            "created_at": datetime.utcnow().isoformat(),
            "data_summary": {
                "weight_recorded": today_weight is not None,
                "meals_recorded": sum(
                    [
                        meal_summary["breakfast"],
                        meal_summary["lunch"],
                        meal_summary["dinner"],
                    ]
                ),
                "total_calories": meal_summary["total_calories"],
                "exercise_minutes": total_exercise_minutes,
                "water_ml": total_water_ml,
            },
        }

        # 9. ç¼“å­˜å»ºè®®ï¼ˆä½¿ç”¨å¸¦å¤§å°é™åˆ¶çš„æ–°å‡½æ•°ï¼‰
        _set_cached_suggestion(cache_key, suggestion)

        return {"success": True, "suggestion": suggestion, "cached": False}

    except Exception as e:
        logger.warning("ç”Ÿæˆæ¯æ—¥å»ºè®®å¤±è´¥: %s", e)
        # è¿”å›é»˜è®¤å»ºè®®
        return {
            "success": True,
            "suggestion": {
                "id": f"default_{current_user.id}",
                "content": "åšæŒè®°å½•æ˜¯å‡é‡çš„ç¬¬ä¸€æ­¥ï¼Œä»Šå¤©ä¹Ÿè¦è®°å¾—è®°å½•ä½“é‡å“¦ï¼ğŸ’ª",
                "type": "general",
                "priority": "medium",
                "action_text": "è®°å½•ä½“é‡",
                "action_type": "navigate",
                "action_target": "weight.html",
                "created_at": datetime.utcnow().isoformat(),
            },
            "cached": False,
            "fallback": True,
        }


# ============ LangChain é›†æˆè·¯ç”± ============


@router.post("/send-langchain")
async def send_message_langchain(
    request_data: dict = Body(...),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    ä½¿ç”¨ LangChain ReAct Agent å‘é€æ¶ˆæ¯ï¼ˆå·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ /send ç«¯ç‚¹ï¼‰

    âš ï¸ DEPRECATED: æ­¤ç«¯ç‚¹å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ /send ç«¯ç‚¹ã€‚
    /send ç«¯ç‚¹ç°åœ¨ä½¿ç”¨ç›¸åŒçš„ LangChain Agent å®ç°ã€‚

    æ–°æ¶æ„ï¼š
    - æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆè®°å½•ä½“é‡ã€åˆ†æé¤é£Ÿç­‰ï¼‰
    - ä¸‰å±‚è®°å¿†ç®¡ç†
    - å‘é‡æ£€ç´¢é•¿æœŸè®°å¿†

    è¿”å› Agent çš„å›å¤ï¼ˆå¸¦å·¥å…·è°ƒç”¨è¿½è¸ªï¼‰
    """
    try:
        # ä»è¯·æ±‚æ•°æ®ä¸­æå–å‚æ•°
        content = request_data.get("content", "")
        image_url = request_data.get("image_url", "")
        msg_type = request_data.get("msg_type", "text")

        # å¦‚æœæ²¡æœ‰æ–‡æœ¬å†…å®¹ä½†æœ‰å›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤æç¤º
        if not content and image_url:
            content = "è¯·å¸®æˆ‘åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„é£Ÿç‰©"

        if not content:
            raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")

        # æ„å»ºæ¶ˆæ¯å†…å®¹
        full_content = content
        if image_url:
            full_content = f"{content}\n[å›¾ç‰‡:{image_url}]"

        # 1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        message = ChatHistory(
            user_id=current_user.id,
            role=MessageRole.USER,
            content=full_content,
            msg_type=MessageType(msg_type),
            meta_data={"image_url": image_url} if image_url else None,
            created_at=datetime.utcnow(),
        )
        db.add(message)
        await db.commit()

        # 2. è°ƒç”¨ LangChain Agentï¼ˆå¸¦ fallbackï¼‰
        try:
            from services.langchain.agents import chat_with_agent
            from services.langchain.memory import save_to_memory

            # è°ƒç”¨ Agent
            result = await chat_with_agent(
                user_id=current_user.id, db=db, message=full_content
            )

            assistant_reply = result.get("response", "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ã€‚")
            intermediate_steps = result.get("intermediate_steps", [])

            # è®°å½•æ—¥å¿—
            logger.info(
                f"LangChain Agent - User: {current_user.id}, Steps: {len(intermediate_steps)}"
            )

        except Exception as agent_error:
            # Fallback åˆ°æ—§ç³»ç»Ÿ
            logger.warning(f"LangChain failed, falling back: {agent_error}")

            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            system_prompt = await build_system_prompt(current_user, db)
            recent_context = await get_recent_context(current_user.id, limit=5, db=db)

            messages = [
                {"role": "system", "content": system_prompt},
                *recent_context,
                {"role": "user", "content": content},
            ]

            # è°ƒç”¨æ—§ AI æœåŠ¡
            response = await call_ai_with_retry(messages)

            if response.error:
                raise Exception(response.error)

            assistant_reply = response.content
            intermediate_steps = [{"type": "fallback", "from": "legacy_ai"}]

        # 3. ä¿å­˜ Agent å›å¤
        reply_message = ChatHistory(
            user_id=current_user.id,
            role=MessageRole.ASSISTANT,
            content=assistant_reply,
            msg_type=MessageType.TEXT,
            created_at=datetime.utcnow(),
        )
        db.add(reply_message)
        await db.commit()

        def format_step(step):
            """æ ¼å¼åŒ–ä¸­é—´æ­¥éª¤"""
            if hasattr(step, "tool") and hasattr(step, "tool_input"):
                return {
                    "tool": step.tool,
                    "input": step.tool_input,
                    "output": getattr(step, "tool_output", "")
                    or getattr(step, "log", ""),
                }
            elif hasattr(step, "return_values"):
                return {
                    "tool": "final",
                    "input": "",
                    "output": step.return_values.get("output", ""),
                }
            else:
                return {
                    "tool": str(type(step).__name__),
                    "input": "",
                    "output": str(step),
                }

        return {
            "success": True,
            "data": {
                "content": assistant_reply,
                "role": "assistant",
                "timestamp": datetime.utcnow().isoformat(),
                "model": "langchain-react-agent",
                "intermediate_steps": [format_step(step) for step in intermediate_steps]
                if intermediate_steps
                else [],
            },
        }

    except Exception as e:
        logger.error(f"LangChain chat error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}",
        )


@router.post("/memory/search")
async def search_user_memory(
    request_data: dict = Body(...),
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db),
):
    """
    æœç´¢ç”¨æˆ·é•¿æœŸè®°å¿†ï¼ˆå‘é‡æ£€ç´¢ï¼‰

    Body:
    - query: æŸ¥è¯¢æ–‡æœ¬
    - category: å¯é€‰ï¼ŒæŒ‰ç±»åˆ«è¿‡æ»¤
    - k: è¿”å›æ•°é‡ï¼Œé»˜è®¤5
    """
    try:
        query = request_data.get("query", "")
        category = request_data.get("category")
        k = request_data.get("k", 5)

        if not query:
            raise HTTPException(status_code=400, detail="æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º")

        # æœç´¢è®°å¿†
        from services.langchain.memory import get_user_memory

        memory = await get_user_memory(current_user.id, db)
        results = await memory.search_memory(query, k=k)

        return {"success": True, "query": query, "results": results}

    except Exception as e:
        logger.error(f"Memory search error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æœç´¢è®°å¿†æ—¶å‡ºé”™: {str(e)}",
        )


@router.post("/memory/clear")
async def clear_user_memory(current_user: User = Depends(get_current_user)):
    """
    æ¸…ç©ºç”¨æˆ·è®°å¿†ï¼ˆçŸ­æœŸ+ä¸­æœŸ+é•¿æœŸï¼‰
    """
    try:
        from services.langchain.memory import MemoryManager

        MemoryManager.clear_user_memory(current_user.id)

        return {"success": True, "message": "è®°å¿†å·²æ¸…ç©º"}

    except Exception as e:
        logger.error(f"Clear memory error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¸…ç©ºè®°å¿†æ—¶å‡ºé”™: {str(e)}",
        )


@router.get("/memory/stats")
async def get_memory_stats(current_user: User = Depends(get_current_user)):
    """
    è·å–ç”¨æˆ·è®°å¿†ç»Ÿè®¡
    """
    try:
        from services.vectorstore.chroma_store import get_user_vector_store

        store = get_user_vector_store(current_user.id)

        return {
            "success": True,
            "stats": {
                "vector_documents": store.count_documents(),
                "user_id": current_user.id,
            },
        }

    except Exception as e:
        logger.error(f"Memory stats error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"è·å–è®°å¿†ç»Ÿè®¡æ—¶å‡ºé”™: {str(e)}",
        )
